# 分布式和和消息队列和海量数据处理

## 为什么要使用分布式

1.提高可用性；

2.易扩展功能；

3.易于开发，每个服务都可独立部署，一个开发人员只须负责一个服务开发；

4.便于提升性能；

# Zookeeper

### zookeeper架构（原理）

ZooKeeper 是一个分布式协调服务，核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。

### 服务注册和发现：

Provider和Consumer向Zookeeper注册临时节点，当连接断开时删除相应的注册节点。
Consumer订阅Providers节点的子节点，实时感知Provider的变化情况，实时同步自身的Invoker对象，保证RPC的可用性。

![è¿éåå¾çæè¿°](md\43.png)

#### 选举流程：

1. 每个Follower都向其他节点发送选自身为Leader的Vote投票请求，等待回复；
2. Follower接受到的Vote如果比自身的大时则投票，并更新自身的Vote，否则拒绝投票；
3. 每个Follower中维护着一个投票记录表，当某个节点收到过半的投票时，结束投票并把该Follower选为Leader，投票结束；

#### 同步流程：

（1）Leader等待sever连接；

（2）Follower连接leader，将最大的zxid发送给leader；

（3）Leader根据follower的zxid确定同步点，完成同步后通知follower 已经成为uptodate状态；

（4）Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。

客户端发起的写请求打到follower时的整个流程。1.follower将请求转发给leader。2。leader发起Proposal，把消息广播给所有follower。3.leader等待过半follower写log并返回ack后,将请求提交。4.将这条日志广播给所有follower。5.follower提交并给客户端返回。

### 为什么推荐Zookeeper作注册中心?

Zookeeper的数据模型很简单，有一系列被称为ZNode的数据节点组成，zk将全量数据存储在内存中，可谓是高性能，而且支持集群，可谓高可用，另外支持事件监听。这些特点决定了zk特别适合作为注册中心。

### 持久节点和临时节点

在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。

### 假死脑裂是什么？如何解决？

master节点宕机，选举了新的节点，宕机节点恢复，存在2个master节点。

如何避免：在follower切换的时候不在检查到老的master出现问题后马上切换，而是在休眠一段足够的时间，确保老的master已经获知变更并且做了相关的shutdown清理工作了然后再注册成为master

### zookeeper作用

分布式协调：A 系统发送请求之后可以在 zookeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 zookeeper 那个节点的值，A 立马就可以收到通知，完美解决。

分布式锁：![img](md\44.jpg)

配置信息管理：作dubbo的注册中心

# dubbo

## Dubbo简介

Dubbo是一款高性能、轻量级的开源Java RPC框架，

它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务注册和发现。

## dubbo架构（原理）

![img](md\45.png)

RPC框架主要包括两大功能：一个用于服务端暴露服务，一个用于客户端引用服务。

引用使用动态代理，将方法增强。如何增强？使用通信框架将地址，方法，参数传输到服务端；

服务端使用通信框架接受参数，通过反射调用方法将结果传输给客户端。

## dubbo远程调用原理

1. *client一个线程调用远程接口，生成一个唯一的ID（比如一段随机字符串，UUID等）*
2. 将打包的方法调用信息（如调用的接口名称，方法名称，参数值列表等），和处理结果的回调对象，全部封装在一起，组成一个对象object
3. 向专门存放调用信息的全局*ConcurrentHashMap里面put(ID, object)*
4. 将*ID和打包的方法调用信息封装成一对象，使用netty进行异步通信*
5. 服务端根据获得的对象进行调用返回结果
6. 客户端监听到数据返回之后，将数据放到回调对象中，然后通过回调对象获取返回数据

## dubbo如何刷新服务地址

init方法内首先拼接要订阅的zk的path，拼接完成后dataid为/dubbo/com.books.dubbo.demo.api.GreetingService/providers，然后创建zkclient订阅该dataid对应的path，并且注册监听器，当path下信息变化后会得到最新列表。

![](md\91.png)

## dubbo智能容错机制

- Failover Cluster - 失败自动切换
- Failfast  Cluster - 快速失败
- Failsafe Cluster - 失败安全
- Failback Cluster - 失败自动恢复
- Forking Cluster - 并行调用多个服务提供者

## dubbo负载均衡

![640?](md\46.jpg)

## RPC为什么不用http协议？

1) 传输速度慢，数据包大（Http协议中包含辅助应用信息）

2) 如实时交互，解包慢，服务器压力大。

3) 数据传输安全性差

## dubbo的spi

SPI 全称为 Service Provider Interface，Dubbo 并未使用 Java 原生的 SPI 机制，而是对其进行了增强，使其能够更好的满足需求。

## dubbo如果一个服务宕机会发生什么？如果注册中心挂掉了，系统还能正常运行吗？

![clipboard.png](md\47.png)

# SpringCloud

 *Spring Cloud* 是一套微服务治理的框架,几乎考虑到了微服务治理的方方面面。

## Eureka

Zookeeper保证CP，设计Eureka时遵守的就是AP原则。如果某台Eureka Server宕机，Eureka Client的请求会自动切换到新的Eureka Server节点，当宕机的服务器重新恢复后，Eureka会再次将其纳入到服务器集群管理之中。当节点开始接受客户端请求时，所有的操作都会进行replicateToPeer（节点间复制）操作，将请求复制到其他Eureka Server当前所知的所有节点中。 

## SpringCloud和Dubbo的区别：

1.Spring Cloud与Dubbo天生使用的协议层面不一样,前者是HTTP,后者是TCP（netty）

2.dubbo是一个rpc框架，springCloud是考虑微服务治理的方方面面。

# 分布式

## CAP理论

一致性：保持各节点数据一致；

可用性：就是无论什么情况，都是可以访问的，不会宕机。

分区容忍性：简单可以理解为，多副本的分布存储。

## BASE理论

BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性，但应用可以采用适合的方式达到最终一致性。

基本可用：基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。

软状态：软状态是指允许系统存在中间状态

最终一致性：最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

#### 强一致性：在任意时刻，所有节点中的数据是一样的。

#### 弱一致性：就是不保证在任意时刻任意节点上的同一份数据都是相同的

数据库主库和从库不一致，常见有这么几种优化方案：
（1）业务可以接受，系统不优化
（2）强制读主，高可用主库，用缓存提高读性能
## 分布式锁

#### redis分布式锁：

RedLock算法：

1. 获取当前时间戳，单位是毫秒；
2. 尝试在**大多数节点**上建立一个锁，比如 5 个节点就要求是 3 个节点 `n / 2 + 1`；
3. 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了；
4. 要是锁建立失败了，那么就依次之前建立过的锁删除；
5. 只要别人建立了一把分布式锁，你就得**不断轮询去尝试获取锁**。

缺陷：强依赖于时间，ABCDE，ABC中的B先过期了，BDE就可以获得锁，存在两个锁。

redis普通锁：![](md\84.png)

缺陷：master宕机前写入myLock，salve还没有同步到这把锁，就被切换成了master，另一线程也写入了myLoCK。

#### zk 分布式锁：

#### redis 分布式锁和 zk 分布式锁的对比：

redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。并且redis锁实现了高级锁特性
zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。

## 分布式事务

#### 具体场景

本地事务和发消息如何保证强一致性

![](md\49.jpg)

### TCC 方案

TCC 的全称是：`Try`、`Confirm`、`Cancel`。

- Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行**锁定或者预留**。
- Confirm 阶段：这个阶段说的是在各个服务中**执行实际的操作**。
- Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要**进行补偿**，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）

## 分布式一致性协议

### 2pc

准备阶段
  参与者接收到preCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日记中
执行阶段
  如果各参与者都成功执行了事务操作，那么反馈给协调者Ack响应，同时等待最终指令，提交commit或者终止abort

- **同步阻塞问题**。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
- **单点故障**。一旦协调者发生故障。参与者会一直阻塞下去。
- **数据不一致**。协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。

### 3pc

- A、CanCommit
  协调者向各参与者发送preCommit请求，并进入prepared阶段
- B、PreCommit
  参与者接收到preCommit请求后，会执行事务操作。
- C、DoCommit
  如果各参与者都成功执行了事务操作，那么反馈给协调者Ack响应，同时等待最终指令，提交commit或者终止abort

改善：3PC主要减少阻塞，第三阶段他会默认执行commit，释放资源。

缺点：一致性性和单点仍存在

### paxos

（1）Prepare 准备阶段

Proposer 想发起提案，问各个 Acceptor ：我是否可以发起？

（2）Accept 接受阶段

如果多数 Acceptor 都同意，那么 Proposer 就真正发出自己的提案，请求大家接受。如果多数 Acceptor 都同意了，提案生效。

### raft

Follower A 等待一个随机的竞选超时时间之后，变成竞选者，发送投票给其他节点，超过半数回复后变成Leader，之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。

## 中间件一致性策略

Zookeeper的zab策略脱胎于Paxos算法，默认情况下，zk中写数据时，要有一半以上的从节点写入成功，才算是写入成功。
redis因为是要提升性能，所以直接采用的异步复制，当在Master上写入数据后直接返回，然后把数据快照广播给Slave，让所有的Slaves去执行操作
Rabbitmq主收到一条消息写入本地存储，然后在发起写入从的请求。当所有从写入成功后，主才会给client返回ack说这次写入成功了。
kafka使用的是ISR，只有 High Water Mark 以上的消息才支持 Consumer 读取，同步上使用全同步机制，等全部写入后才返回成功

![](md\89.png)

更少的可容错副本数量。raft当有1/3个节点数目不可用时，服务不可用，kafka中维护的ISR（in-sync-replica），即便副本均不可用，只剩下Leader，也可以对外提供服务

es有三种一致性参数：
1.one：要求我们这个写操作，只要有一个primary shard是active活跃可用的，就可以执行
2.all：要求我们这个写操作，必须所有的primary shard和replica shard都是活跃的，才可以执行这个写操作
3.quorum：默认的值，要求所有的shard中，必须是大部分的shard都是活跃的，可用的，才可以执行这个写操作

# RabbitMq

![](md\90.png)

Broker:它提供一种传输服务,它的角色就是维护一条从生产者到消费者的路线，保证数据能按照指定的方式进行传输, 
Exchange：消息交换机,它指定消息按什么规则,路由到哪个队列。 
Queue:消息的载体,每个消息都会被投到一个或多个队列。 
Binding:绑定，它的作用就是把exchange和queue按照路由规则绑定起来. 
Routing Key:路由关键字,exchange根据这个关键字进行消息投递。 
vhost:虚拟主机,一个broker里可以有多个vhost，用作不同用户的权限分离。例如默认/，可设置为/zeng/1 
Producer:消息生产者,就是投递消息的程序. 
Consumer:消息消费者,就是接受消息的程序. 

#### 为什么使用消息队列

异步、解耦、削峰

#### 交换机（Exchange）

direct(默认)，fanout, topic, 和headers

路由健完全匹配，不处理路由健，路由健模糊匹配，基于消息的header数据匹配；

三种模式 direct fanout topic，在 direct 模式下明显消息发布的性能比其他模式强很多

#### 消息队列的缺点

系统可用性降低：镜像集群

系统复杂度提高：需要考虑消息重复消费、消息丢失、甚至消息顺序性

数据一致性问题

#### RabbitMq集群：

 RabbitMQ 集群分为两种 普通集群 和 镜像集群。

普通集群：A收到消息 B消费，则要从A上去拿；

镜像集群：主从架构；

#### CMQ集群

CMQ中，主要是实现主从集群。Raft 协议强依赖 Leader 节点的可用性来确保集群数据的一致性。数据的流向只能从Leader 节点向 Follower 节点转移。

#### rabbitmq 每个节点是其他节点的完整拷贝吗？为什么？

普通集群则不是；镜像集群是；

#### 如何确保消息不丢失

![img](md\54.png)

#### 保证幂等性（不被重复消费）

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 直接用把offset写入redis中，看是否消费过； 

#### 如何保证消息的顺序性？

如果存在多个消费者，那么就让每个消费者对应一个queue，然后把要发送 的数据全都放到一个queue，这样就能保证所有的数据只到达一个消费者从而保证每个数据到达数据库都是顺序的。

#### 如何防止处理过程中异常而导致的消息丢失？

消费者拉取消息时会指定当前消息的隐藏时间，在隐藏时 间内消费者需要显式的对消息进行确认删除（ACK），消息才会真真的被删除。如果超过隐藏时间没有主动删除， 此条消息将重新对外可见，可以继续消费。三种模式 direct fanout topic，在 direct 模式下明显消息发布的性能比其他模式强很多

#### 百万数据积压

1、先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉。

2、新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量。

3、然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue

4、接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据

5、这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据

6、等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息

# Kafka

#### Kafka简介

一、Kafka是一个分布式消息队列系统，以集群方式存在。

优点：高吞吐率（每秒百万级）；基于多分区多副本实现高容错，并发能力强；易扩展（增加broker）等。

#### Kafka原理

Kafka依赖zk，以集群方式工作，每台机器称为一个brocker，并分别指定brockerId，kafka中同一类型数据以topic形式存在，可对topic进行分区，及指定副本数，例如可将 topic1 分区为3个partition:p0,p1,p2 ;对于每个partition，send(topic, key, data); key&size选择partition；

同一个分组内所有消费者消费一份完整的数据，此时一个分区数据只能被同一消费组的一个消费者消费，而一个消费者可以消费多个分区数据

#### 推还是拉？

推的话消费速度更快，但容易推的消息速度过大，造成comsumer崩溃。

#### Kafka集群Leader选举原理

Kafka的Leader选举是通过在zookeeper上创建/controller临时节点来实现leader选举，并在该节点中写入当前broker的信息 ：{“version”:1,”brokerid”:1,”timestamp”:”1512018424988”} 
利用Zookeeper的强一致性特性，一个节点只能被一个客户端创建成功，创建成功的broker即为leader，即先到先得原则，leader也就是集群中的controller，负责集群中所有大小事务。

#### kafka一致性保证

Kafka通过replica.lag.max.messages和replica.lag.time.max.ms两个参数来度量Follower同步的情况。其中replica.lag.max.messages用于配置Follower可落后的最大消息数量，replica.lag.time.max.ms用于配置Follower和Leader通信的最大时延。超过后会被踢出ISR，直到追上后加回到ISR列表

#### offset保存策略

使用zookeeper来维护offset，kafka 0.9 以前的版本是将offset 存储在zookeeper上的。kafka 0.9 以后，offset的使用了内部的roker来管理。

#### Rebalance分配策略（partion分配给consumer的过程）

1、轮询的方式分配所有的分区

2、按分区数/消费者数来分配。

3、Sticky：首先将分区根据所分配的consumer数量从低到高进行排序，后将最前的分区分给订阅了的且分配了最少分区的消费者。

#### 如何保证消息的顺序性？

生产者生产消息只发到一个partation ，单partation有序，多partation无序，Kafka 只会保证在 Partition 内消息是有序的，而不管全局的情况。

#### 和RabbitMq对比：

在架构模型方面：AMQP：Exchange,Binding,queue、、一般的MQ结构，broker

吞吐量：kafka具有高的吞吐量，内部采用消息的批量处理。rabbitMQ支持对消息的可靠的传递，支持事务，不支持批量的操作；一般RabbitMQ的单机QPS在万级别之内，而Kafka的单机QPS可以维持在十万级别，甚至可以达到百万级。

使用场景：就目前而言，在金融支付领域使用RabbitMQ居多，而在日志处理、大数据等方面Kafka使用居多。

## Nginx

![img](md\60.jpg)

- **多进程：**一个 Master 进程、多个 Worker 进程。
- **Master 进程：**管理 Worker 进程。对外接口：接收外部的操作（信号）；对内转发：根据外部的操作的不同，通过信号管理 Worker；**监控：**监控 Worker 进程的运行状态，Worker 进程异常终止后，自动重启 Worker 进程。
- **Worker 进程：**所有 Worker 进程都是平等的。实际处理：网络请求，由 Worker 进程处理。Worker 进程数量**：**在 nginx.conf 中配置，一般设置为核心数，充分利用 CPU 资源，同时，避免进程数量过多，避免进程竞争 CPU 资源，增加上下文切换的损耗。

### nginx的处理流程

Nginx的IO通常使用epoll，epoll函数使用了I/O复用模型。与I/O阻塞模型比较，I/O复用模型的优势在于可以同时等待多个（而不只是一个）套接字描述符就绪。Nginx的epoll工作流程如下：

1.首先，master 进程接受到信号（如nginx -s reload）后启动，读取配置文件，建好需要listen的socket后，然后再fork出多个woker进程，这样每个work进程都可以去accept这个socket

2.当一个client连接到来时，所有accept的work进程都会受到通知，但只有一个进程可以accept成功，其它的则会accept失败，Nginx提供了一把共享锁accept_mutex来保证同一时刻只有一个work进程在accept连接，从而解决惊群问题

3.当一个worker进程accept这个连接后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完成的请求就结束了

## Maven

mvn clean package依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)等７个阶段。
mvn clean install依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)、install等8个阶段。
mvn clean deploy依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)、install、deploy等９个阶段。

主要区别如下:
package命令完成了项目编译、单元测试、打包功能，但没有把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库

install命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库，但没有布署到远程maven私服仓库

deploy命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库.

# 海量数据处理

### 1.海量日志数据，提取出某日访问百度次数最多的那个IP

1. 分而治之/hash映射：针对数据太大，内存受限，只能是把大文件化成(取模映射)小文件；按照IP地址的Hash(IP)%1000值，把海量IP日志分别存储到1000个小文件中。这样，每个小文件最多包含4MB个IP地址
2. hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。

### 2.寻找热门查询，300万个查询字符串中统计最热门的10个查询

一个字符串1~255字节；300 * 10^4 * 255/ 2^30=0.75G直接放在内存中hashmap处理；后进行堆排序；

### 3.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

分为1000个小文件，每个都用hashmap统计频率，取出最高的100个，然后堆排序；

### 4.海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。

顺序读取10个文件，hashmap每台机器取前10，然后堆排序；

### 5.有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。

顺序读取10个文件，按照hash%10写入到另外10个文件，hashmap统计，快速排序

### 6.给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

a，b文件分别遍历按hash分给100个小文件，hashset对比是否有相同的url；

### 7.在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。

采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存

### 8.给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

### 布隆过滤器

![img](md\61.jpg)



那么我们可以说 “baidu” 存在了么？答案是不可以，只能是 “baidu” 这个值可能存在。











