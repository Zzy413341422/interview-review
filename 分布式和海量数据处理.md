# 分布式和和消息队列和海量数据处理

# 集群/分布式/微服务/SOA是什么？

**集群**：同一个业务，部署在多个服务器上

**分布式**：一个业务分拆多个子业务，部署在不同的服务器上

**微服务**：

## 为什么要使用分布式

1.提高可用性；

2.易扩展功能；

3.易于开发，每个服务都可独立部署，一个开发人员只须负责一个服务开发；

4.便于提升性能；

# Zookeeper

### zookeeper架构（原理）

ZooKeeper 是一个分布式协调服务，核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。

### 服务注册和发现：

Provider和Consumer向Zookeeper注册临时节点，当连接断开时删除相应的注册节点。
Consumer订阅Providers节点的子节点，实时感知Provider的变化情况，实时同步自身的Invoker对象，保证RPC的可用性。

![è¿éåå¾çæè¿°](md\43.png)

#### 选举流程：

1. 每个Follower都向其他节点发送选自身为Leader的Vote投票请求，等待回复；
2. Follower接受到的Vote如果比自身的大时则投票，并更新自身的Vote，否则拒绝投票；
3. 每个Follower中维护着一个投票记录表，当某个节点收到过半的投票时，结束投票并把该Follower选为Leader，投票结束；

#### 同步流程：

（1）Leader等待sever连接；

（2）Follower连接leader，将最大的zxid发送给leader；

（3）Leader根据follower的zxid确定同步点，完成同步后通知follower 已经成为uptodate状态；

（4）Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。

### 为什么推荐Zookeeper作注册中心?

Zookeeper的数据模型很简单，有一系列被称为ZNode的数据节点组成，zk将全量数据存储在内存中，可谓是高性能，而且支持集群，可谓高可用，另外支持事件监听。这些特点决定了zk特别适合作为注册中心。

### 持久节点和临时节点

在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。

### 假死脑裂是什么？如何解决？

master节点宕机，选举了新的节点，宕机节点恢复，存在2个master节点。

如何避免：在follower切换的时候不在检查到老的master出现问题后马上切换，而是在休眠一段足够的时间，确保老的master已经获知变更并且做了相关的shutdown清理工作了然后再注册成为master

### zookeeper作用

分布式协调：A 系统发送请求之后可以在 zookeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 zookeeper 那个节点的值，A 立马就可以收到通知，完美解决。

分布式锁：![img](md\44.jpg)

配置信息管理：作dubbo的注册中心

# dubbo

## Dubbo简介

Dubbo是一款高性能、轻量级的开源Java RPC框架，

它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务注册和发现。

## dubbo架构（原理）

![img](md\45.png)

RPC框架主要包括两大功能：一个用于服务端暴露服务，一个用于客户端引用服务。

引用使用动态代理，将方法增强。如何增强？使用通信框架将地址，方法，参数传输到服务端；

服务端使用通信框架接受参数，通过反射调用方法将结果传输给客户端。

## dubbo远程调用原理

1. *client一个线程调用远程接口，生成一个唯一的ID（比如一段随机字符串，UUID等）*

2. 将打包的方法调用信息（如调用的接口名称，方法名称，参数值列表等），和处理结果的回调对象，全部封装在一起，组成一个对象object

3. 向专门存放调用信息的全局*ConcurrentHashMap里面put(ID, object)*

4. 将*ID和打包的方法调用信息封装成一对象，使用netty进行异步通信*

5. 服务端根据获得的对象进行调用返回结果

6. 客户端监听到数据返回之后，将数据放到回调对象中，然后通过回调对象获取返回数据

## dubbo智能容错机制

- Failover Cluster - 失败自动切换
- Failfast  Cluster - 快速失败
- Failsafe Cluster - 失败安全
- Failback Cluster - 失败自动恢复
- Forking Cluster - 并行调用多个服务提供者

## dubbo负载均衡

![640?](md\46.jpg)

## RPC为什么不用http协议？

1) 传输速度慢，数据包大（Http协议中包含辅助应用信息）

2) 如实时交互，解包慢，服务器压力大。

3) 数据传输安全性差

## dubbo的spi

SPI 全称为 Service Provider Interface，Dubbo 并未使用 Java 原生的 SPI 机制，而是对其进行了增强，使其能够更好的满足需求。

## dubbo如果一个服务宕机会发生什么？如果注册中心挂掉了，系统还能正常运行吗？

![clipboard.png](md\47.png)

# SpringCloud

 *Spring Cloud* 是一套微服务治理的框架,几乎考虑到了微服务治理的方方面面。

## Eureka

Zookeeper保证CP，设计Eureka时遵守的就是AP原则。如果某台Eureka Server宕机，Eureka Client的请求会自动切换到新的Eureka Server节点，当宕机的服务器重新恢复后，Eureka会再次将其纳入到服务器集群管理之中。当节点开始接受客户端请求时，所有的操作都会进行replicateToPeer（节点间复制）操作，将请求复制到其他Eureka Server当前所知的所有节点中。

## Zuul

外层调用都必须通过API网关，使得将维护服务实例的工作交给了服务治理框架自动完成。
在API网关服务上进行统一调用来对微服务接口做前置过滤，以实现对微服务接口的拦截和校验。

## Config

简单来说，使用Spring Cloud Config就是将配置文件放到统一的位置管理(比如GitHub)，客户端通过接口去获取这些配置文件。 

## SpringCloud和Dubbo的区别：

1.Spring Cloud与Dubbo天生使用的协议层面不一样,前者是HTTP,后者是TCP（netty）

2.dubbo是一个rpc框架，springCloud是考虑微服务治理的方方面面。

# 分布式

## CAP理论

一致性：保持各节点数据一致；

可用性：就是无论什么情况，都是可以访问的，不会宕机。

分区容忍性：简单可以理解为，多副本的分布存储。

## BASE理论

BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性，但应用可以采用适合的方式达到最终一致性。

基本可用：基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。

软状态：软状态是指允许系统存在中间状态

最终一致性：最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

#### 强一致性：在任意时刻，所有节点中的数据是一样的。

#### 弱一致性：就是不保证在任意时刻任意节点上的同一份数据都是相同的

## 分布式锁

#### redis分布式锁：

RedLock算法：

1. 获取当前时间戳，单位是毫秒；
2. 尝试在**大多数节点**上建立一个锁，比如 5 个节点就要求是 3 个节点 `n / 2 + 1`；
3. 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了；
4. 要是锁建立失败了，那么就依次之前建立过的锁删除；
5. 只要别人建立了一把分布式锁，你就得**不断轮询去尝试获取锁**。

#### zk 分布式锁：

#### redis 分布式锁和 zk 分布式锁的对比：

redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。
zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。

## 分布式事务

![](md\49.jpg)

### TCC 方案

TCC 的全称是：`Try`、`Confirm`、`Cancel`。

- Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行**锁定或者预留**。
- Confirm 阶段：这个阶段说的是在各个服务中**执行实际的操作**。
- Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要**进行补偿**，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）

## 分布式会话

tomcat + redis

spring session + redis

## 分布式一致性协议

### 2pc

![clipboard.png](md\50.png)

- **同步阻塞问题**。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
- **单点故障**。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。
- **数据不一致**。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。

### 3pc

- A、CanCommit
  协调者向各参与者发送preCommit请求，并进入prepared阶段
- B、PreCommit
  参与者接收到preCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日记中
- C、DoCommit
  如果各参与者都成功执行了事务操作，那么反馈给协调者Ack响应，同时等待最终指令，提交commit或者终止abort

改善：3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。

缺点：但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。

### paxos

多个proposer发请提议[id,value]，acceptor接受最新id的提议并把之前保留的提议相应给properser。当超过半数的accetor返回某个提议给对应的properser，propeser认为可以接受该提议，于是广播给每个acceptor，acceptor发现该提议和自己保存的一致，于是接受该提议并且learner同步该提议。

### raft

Follower A 等待一个随机的竞选超时时间之后，变成竞选者，发送投票给其他节点，超过半数回复后变成Leader，之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。

# RabbitMq

#### 为什么使用消息队列

异步：假设A系统接收一个请求，需要在自己本地写库执行SQL，然后需要调用BCD三个系统的接口。

假设自己本地写库要3ms，调用BCD三个系统分别要300ms、450ms、200ms。

那么最终请求总延时是3 + 300 + 450 + 200 = 953ms，接近1s，可能用户会感觉太慢了。

![img](md\51.jpg)

![img](md\52.jpg)

解耦：

![img](md\53.jpg)

削峰；

#### 交换机（Exchange）

direct(默认)，fanout, topic, 和headers

路由健完全匹配，不处理路由健，路由健模糊匹配，基于消息的header数据匹配；

三种模式 direct fanout topic，在 direct 模式下明显消息发布的性能比其他模式强很多

#### 消息队列的缺点

系统可用性降低：镜像集群

系统复杂度提高：需要考虑消息重复消费、消息丢失、甚至消息顺序性

数据一致性问题

#### RabbitMq集群：

 RabbitMQ 集群分为两种 普通集群 和 镜像集群。

普通集群：A收到消息 B消费，则要从A上去拿；

镜像集群：主从架构；

#### CMQ集群

CMQ中，主要是实现主从集群。Raft 协议强依赖 Leader 节点的可用性来确保集群数据的一致性。数据的流向只能从Leader 节点向 Follower 节点转移。

#### rabbitmq 每个节点是其他节点的完整拷贝吗？为什么？

普通集群则不是；镜像集群是；

#### 如何确保消息不丢失

![img](md\54.png)

#### 保证幂等性（不被重复消费）

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 直接用把offset写入redis中，看是否消费过； 

#### 如何保证消息的顺序性？

如果存在多个消费者，那么就让每个消费者对应一个queue，然后把要发送 的数据全都放到一个queue，这样就能保证所有的数据只到达一个消费者从而保证每个数据到达数据库都是顺序的。

#### 如何防止处理过程中异常而导致的消息丢失？

消费者拉取消息时会指定当前消息的隐藏时间，在隐藏时 间内消费者需要显式的对消息进行确认删除（ACK），消息才会真真的被删除。如果超过隐藏时间没有主动删除， 此条消息将重新对外可见，可以继续消费。三种模式 direct fanout topic，在 direct 模式下明显消息发布的性能比其他模式强很多

#### 百万数据积压

1、先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉。

2、新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量。

3、然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue

4、接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据

5、这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据

6、等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息

# Kafka

#### Kafka简介

一、Kafka是一个分布式消息队列系统，以集群方式存在。

优点：高吞吐率（每秒百万级）；基于多分区多副本实现高容错，并发能力强；易扩展（增加broker）等。

#### Kafka原理

Kafka依赖zk，以集群方式工作，每台机器称为一个brocker，并分别指定brockerId，kafka中同一类型数据以topic形式存在，可对topic进行分区，及指定副本数，例如可将 topic1 分区为3个partition:p0,p1,p2 ;对于每个partition，send(topic, key, data); key&size选择partition；

同一个分组内所有消费者消费一份完整的数据，此时一个分区数据只能被同一消费组的一个消费者消费，而一个消费者可以消费多个分区数据

#### 推还是拉？

推的话消费速度更快，但容易推的消息速度过大，造成comsumer崩溃。

#### Kafka集群Leader选举原理

Kafka的Leader选举是通过在zookeeper上创建/controller临时节点来实现leader选举，并在该节点中写入当前broker的信息 ：{“version”:1,”brokerid”:1,”timestamp”:”1512018424988”} 
利用Zookeeper的强一致性特性，一个节点只能被一个客户端创建成功，创建成功的broker即为leader，即先到先得原则，leader也就是集群中的controller，负责集群中所有大小事务。

#### kafka一致性保证

Kafka通过replica.lag.max.messages和replica.lag.time.max.ms两个参数来度量Follower同步的情况。其中replica.lag.max.messages用于配置Follower可落后的最大消息数量，replica.lag.time.max.ms用于配置Follower和Leader通信的最大时延。超过后会被踢出ISR，直到追上后加回到ISR列表

#### offset保存策略

使用zookeeper来维护offset，kafka 0.9 以前的版本是将offset 存储在zookeeper上的。kafka 0.9 以后，offset的使用了内部的roker来管理。

#### Rebalance分配策略（partion分配给consumer的过程）

1、轮询的方式分配所有的分区

2、按分区数/消费者数来分配。

3、Sticky：首先将分区根据所分配的consumer数量从低到高进行排序，后将最前的分区分给订阅了的且分配了最少分区的消费者。

#### 如何保证消息的顺序性？

生产者生产消息只发到一个partation ，单partation有序，多partation无序，Kafka 只会保证在 Partition 内消息是有序的，而不管全局的情况。

#### 和RabbitMq对比：

在架构模型方面：AMQP：Exchange,Binding,queue、、一般的MQ结构，broker

吞吐量：kafka具有高的吞吐量，内部采用消息的批量处理。rabbitMQ支持对消息的可靠的传递，支持事务，不支持批量的操作；一般RabbitMQ的单机QPS在万级别之内，而Kafka的单机QPS可以维持在十万级别，甚至可以达到百万级。

使用场景：就目前而言，在金融支付领域使用RabbitMQ居多，而在日志处理、大数据等方面Kafka使用居多。

## Nginx

![img](md\60.jpg)

- **多进程：**一个 Master 进程、多个 Worker 进程。
- **Master 进程：**管理 Worker 进程。对外接口：接收外部的操作（信号）；对内转发：根据外部的操作的不同，通过信号管理 Worker；**监控：**监控 Worker 进程的运行状态，Worker 进程异常终止后，自动重启 Worker 进程。
- **Worker 进程：**所有 Worker 进程都是平等的。实际处理：网络请求，由 Worker 进程处理。Worker 进程数量**：**在 nginx.conf 中配置，一般设置为核心数，充分利用 CPU 资源，同时，避免进程数量过多，避免进程竞争 CPU 资源，增加上下文切换的损耗。

### nginx的处理流程

Nginx的IO通常使用epoll，epoll函数使用了I/O复用模型。与I/O阻塞模型比较，I/O复用模型的优势在于可以同时等待多个（而不只是一个）套接字描述符就绪。Nginx的epoll工作流程如下：

1.首先，master 进程接受到信号（如nginx -s reload）后启动，读取配置文件，建好需要listen的socket后，然后再fork出多个woker进程，这样每个work进程都可以去accept这个socket

2.当一个client连接到来时，所有accept的work进程都会受到通知，但只有一个进程可以accept成功，其它的则会accept失败，Nginx提供了一把共享锁accept_mutex来保证同一时刻只有一个work进程在accept连接，从而解决惊群问题

3.当一个worker进程accept这个连接后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完成的请求就结束了

## Maven

mvn clean package依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)等７个阶段。
mvn clean install依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)、install等8个阶段。
mvn clean deploy依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)、install、deploy等９个阶段。

主要区别如下:
package命令完成了项目编译、单元测试、打包功能，但没有把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库

install命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库，但没有布署到远程maven私服仓库

deploy命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库.

# 海量数据处理

### 1.海量日志数据，提取出某日访问百度次数最多的那个IP

1. 分而治之/hash映射：针对数据太大，内存受限，只能是把大文件化成(取模映射)小文件；按照IP地址的Hash(IP)%1000值，把海量IP日志分别存储到1000个小文件中。这样，每个小文件最多包含4MB个IP地址
2. hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。

### 2.寻找热门查询，300万个查询字符串中统计最热门的10个查询

一个字符串1~255字节；300 * 10^4 * 255/ 2^30=0.75G直接放在内存中hashmap处理；后进行堆排序；

### 3.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

分为1000个小文件，每个都用hashmap统计频率，取出最高的100个，然后堆排序；

### 4.海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。

顺序读取10个文件，hashmap每台机器取前10，然后堆排序；

### 5.有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。

顺序读取10个文件，按照hash%10写入到另外10个文件，hashmap统计，快速排序

### 6.给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

a，b文件分别遍历按hash分给100个小文件，hashset对比是否有相同的url；

### 7.在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。

采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存

### 8.给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

### 布隆过滤器

![img](md\61.jpg)



那么我们可以说 “baidu” 存在了么？答案是不可以，只能是 “baidu” 这个值可能存在。











