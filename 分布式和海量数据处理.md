# 分布式和和消息队列和海量数据处理

# 集群/分布式/微服务/SOA是什么？

**集群**：同一个业务，部署在多个服务器上

**分布式**：一个业务分拆多个子业务，部署在不同的服务器上

**微服务**：Microservices - also known as the microservice architecture - is an architectural style that structures an application as a collection of services that are

- Highly maintainable and testable
- Loosely coupled
- Independently deployable
- Organized around business capabilities.

## 为什么要使用分布式

1.提高可用性；

2.易扩展功能；

3.易于开发，每个服务都可独立部署，一个开发人员只须负责一个服务开发；

4.便于提升性能；

# Zookeeper

#### zookeeper架构（原理）

ZooKeeper 是一个分布式协调服务，核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。

#### 服务注册和发现：

Provider和Consumer向Zookeeper注册临时节点，当连接断开时删除相应的注册节点。
Consumer订阅Providers节点的子节点，实时感知Provider的变化情况，实时同步自身的Invoker对象，保证RPC的可用性。

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180609174042958?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI3NTI5OTE3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

#### 选举流程：

1. 每个Follower都向其他节点发送选自身为Leader的Vote投票请求，等待回复；
2. Follower接受到的Vote如果比自身的大时则投票，并更新自身的Vote，否则拒绝投票；
3. 每个Follower中维护着一个投票记录表，当某个节点收到过半的投票时，结束投票并把该Follower选为Leader，投票结束；

#### 同步流程：

（1）Leader等待sever连接；

（2）Follower连接leader，将最大的zxid发送给leader；

（3）Leader根据follower的zxid确定同步点，完成同步后通知follower 已经成为uptodate状态；

（4）Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。

### 为什么推荐Zookeeper作注册中心?

Zookeeper的数据模型很简单，有一系列被称为ZNode的数据节点组成，zk将全量数据存储在内存中，可谓是高性能，而且支持集群，可谓高可用，另外支持事件监听。这些特点决定了zk特别适合作为注册中心。

### 持久节点和临时节点

在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。

### zookeeper作用

分布式协调：A 系统发送请求之后可以在 zookeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 zookeeper 那个节点的值，A 立马就可以收到通知，完美解决。

分布式锁：

配置信息管理：作dubbo的注册中心

# dubbo

## Dubbo简介

Dubbo是一款高性能、轻量级的开源Java RPC框架，

它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务注册和发现。

## dubbo架构（原理）

![img](https://img-blog.csdn.net/20170922152838774?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3dkMTE1NDk3ODM1Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

RPC框架主要包括两大功能：一个用于服务端暴露服务，一个用于客户端引用服务。

引用使用动态代理，将方法增强。如何增强？使用通信框架将地址，方法，参数传输到服务端；

服务端使用通信框架接受参数，通过反射调用方法将结果传输给客户端。

## dubbo的集群容错机制

- 这里的 `Invoker` 是 `Provider` 的一个可调用 `Service` 的抽象，`Invoker` 封装了 `Provider` 地址及 `Service` 接口信息
- `Directory` 代表多个 `Invoker`，可以把它看成 `List<Invoker>` ，但与 `List` 不同的是，它的值可能是动态变化的，比如注册中心推送变更
- `Cluster` 将 `Directory` 中的多个 `Invoker` 伪装成一个 `Invoker`，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个
- `Router` 负责从多个 `Invoker` 中按路由规则选出子集，比如读写分离，应用隔离等
- `LoadBalance` 负责从多个 `Invoker` 中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180708230808373?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ByZXN0aWdlZGluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/TNUwKhV0JpSThKEqVKRNPY8ib8Be6Tw1xPnqibAiclAa89ALLe5Xe4eek328q4UXmRAPxgHrPia0924WVh576nUib2A/640?)

## dubbo负载均衡

## ![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/TNUwKhV0JpSThKEqVKRNPY8ib8Be6Tw1xJtoAAQLNVSibMocrg0aiaDWSbEMaIA6omGvd9h8WEZib2KBQKjAWw3PjQ/640?)

## RPC为什么不用http协议？

1) 传输速度慢，数据包大（Http协议中包含辅助应用信息）

2) 如实时交互，解包慢，服务器压力大。

3) 数据传输安全性差

## dubbo的spi

SPI 全称为 Service Provider Interface，Dubbo 并未使用 Java 原生的 SPI 机制，而是对其进行了增强，使其能够更好的满足需求。

## dubbo如果一个服务宕机会发生什么？如果注册中心挂掉了，系统还能正常运行吗？

![clipboard.png](https://segmentfault.com/img/bVGh1z?w=1218&h=284)

# SpringCloud

 *Spring Cloud* 是一套微服务治理的框架,几乎考虑到了微服务治理的方方面面。

## Eureka

#### Eureka高可用

Zookeeper保证CP，设计Eureka时遵守的就是AP原则。如果某台Eureka Server宕机，Eureka Client的请求会自动切换到新的Eureka Server节点，当宕机的服务器重新恢复后，Eureka会再次将其纳入到服务器集群管理之中。当节点开始接受客户端请求时，所有的操作都会进行replicateToPeer（节点间复制）操作，将请求复制到其他Eureka Server当前所知的所有节点中。

## Ribbon

客户端负载均衡(Ribbon)
服务实例的清单在客户端，客户端进行负载均衡算法分配。
服务端负载均衡(Nginx)
服务实例的清单在服务端，服务器进行负载均衡算法分配

## Hystrix

熔断机制：

当失败率达到阀值自动触发降级。

隔离模式:

线程池隔离模式:流量洪峰来临时，处理不完可将数据存储到线程池队里慢慢处理

信号量隔离模式:流量洪峰来临时，处理的线程超过数量，其他的请求会直接返回，不继续去请求依赖的服务

降级机制: 快速模式、故障转移、主次模式（回退到老代码）。

## Feign

整合了 Spring Cloud Ribbon 与 Spring Cloud Hystrix, 除了整合这两者的强大功能之外，它还提 供了声明式的服务调用(不再通过RestTemplate)。

## Zuul（网关）

外层调用都必须通过API网关，使得将维护服务实例的工作交给了服务治理框架自动完成。
在API网关服务上进行统一调用来对微服务接口做前置过滤，以实现对微服务接口的拦截和校验。

#### 和ribbon负载均衡的不同？

zuul是对外暴露的唯一接口相当于路由的是controller的请求，而Ribbon和Fegin路由了service的请求

## Config

简单来说，使用Spring Cloud Config就是将配置文件放到统一的位置管理(比如GitHub)，客户端通过接口去获取这些配置文件。

## Bus

当git文件更改的时候，通过pc端用post 向端口为8882的config-client发送请求/bus/refresh／；此时8882端口会发送一个消息，由消息总线向其他服务传递，从而使整个微服务集群都达到更新配置文件。

## Sleuth

服务的追踪

## Hystrix Dashboard

Hystrix还提供了近实时的监控，Hystrix会实时、累加地记录所有关于HystrixCommand的执行信息，包括每秒执行多少请求多少成功，多少失败等。

## SpringCloud和Dubbo的区别：

1.Spring Cloud与Dubbo天生使用的协议层面不一样,前者是HTTP,后者是TCP（netty）

2.dubbo是一个rpc框架，springCloud是考虑微服务治理的方方面面。

# 分布式

## CAP理论

一致性：保持各节点数据一致；

可用性：就是无论什么情况，都是可以访问的，不会宕机。

分区容忍性：简单可以理解为，多副本的分布存储。

## 分布式锁

#### redis分布式锁：

RedLock算法：

1. 获取当前时间戳，单位是毫秒；
2. 尝试在**大多数节点**上建立一个锁，比如 5 个节点就要求是 3 个节点 `n / 2 + 1`；
3. 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了；
4. 要是锁建立失败了，那么就依次之前建立过的锁删除；
5. 只要别人建立了一把分布式锁，你就得**不断轮询去尝试获取锁**。

#### zk 分布式锁：

zk 分布式锁，其实可以做的比较简单，就是某个节点尝试创建临时 znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能**注册个监听器**监听这个锁。释放锁就是删除这个 znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新加锁。

#### redis 分布式锁和 zk 分布式锁的对比：

redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。
zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。

## 分布式事务

### 2pc

将事务的提交过程分为：准备阶段和提交阶段。事务的发起者称协调者，事务的执行者称参与者。

准备阶段：各参与者执行事务操作，但不提交事务。提交阶段：所有参与者均反馈YES时，即提交事务。

#### **2PC的缺陷**

1、同步阻塞：最大的问题即同步阻塞，即：所有参与事务的逻辑均处于阻塞状态。
2、单点：协调者存在单点问题，如果协调者出现故障，参与者将一直处于锁定状态。

3、**数据不一致性**：但是因为网络问题该通知仅被一部分参与者所收到并执行了commit操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。

### **3PC**

3PC，三阶段提交协议，是2PC的改进版本，即将事务的提交过程分为CanCommit、PreCommit、do Commit三个阶段来进行处理。

PreCommit：参与者收到PreCommit请求后，执行事务操作，但不提交事务。

do Commit：看PreCommit执行结果，成功则提交，失败或等待超时则回滚，参与者发送结果但未收到ack则自动提交。

### **3PC的优点和缺陷**

优点：降低了阻塞范围，引入了超时机制，在等待超时后协调者或参与者会中断事务。

缺陷：数据不一致：即在参与者收到PreCommit请求后等待最终指令，如果此时协调者无法与参与者正常通信，会导致参与者继续提交事务，造成数据不一致。

## 分布式会话

#### tomcat + redis

#### spring session + redis

## 分布式一致性协议

https://www.cnblogs.com/zhang-qc/p/8688258.html

paxos：多个proposer发请提议[id,value]，acceptor接受最新id的提议并把之前保留的提议相应给properser。当超过半数的accetor返回某个提议给对应的properser，propeser认为可以接受该提议，于是广播给每个acceptor，acceptor发现该提议和自己保存的一致，于是接受该提议并且learner同步该提议。

raft：Follower A 等待一个随机的竞选超时时间之后，变成竞选者，发送投票给其他节点，超过半数回复后变成Leader，之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。

## 缓存一致性

- 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。
- 更新的时候，**先更新数据库，然后再删除缓存**。

#### 为什么要删除缓存而不是更新缓存？

假如增删改100次后，则更新了100次，而删除缓存则只删除了一次。

# Kafka

#### 为什么使用消息队列

**解耦**、**异步**、**削峰**。

#### Kafka简介

一、Kafka是一个分布式消息队列系统，以集群方式存在。

优点：高吞吐率（每秒百万级）；基于多分区多副本实现高容错，并发能力强；易扩展（增加blocker）等。

#### Kafka原理

Kafka依赖zk，以集群方式工作，每台机器称为一个brocker，并分别指定brockerId，kafka中同一类型数据以topic形式存在，可对topic进行分区，及指定副本数，例如可将 topic1 分区为3个partition:p0,p1,p2 ;对于每个partition，send(topic, key, data); key&size选择partition；有多个副本，会选出一个leader partition对外接收请求（只有leader partition会接收外部请求，其他follower partition只负责同步leader数据，不接收外部请求，包括读请求，这里与zk不同，zk的follower会接收读请求）。producer向broker发送消息，consumer group拉取消息；![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20170816194851267?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGxnZW4xNTczODc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 1个partition只能被1个consumer gourp内的一个comsumer拥有

![1558762694826](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\1558762694826.png)

#### kafka需要向zookpeer配置什么内容

**kafka对与zookeeper是强依赖的**，管理broker、注册topic、Zookeeper管理consumer的offset跟踪当前消费的offset。

#### kafka消息交付语义

- At most once: 消息可能会丢，但绝不会重复传输
- At least once：消息绝不会丢，但可能会重复传输
- Exactly once：每条消息肯定会被传输一次且仅传输一次

#### Kafka如何解决数据堆积

加partion，加consumer gourp；

#### Kafka集群Leader选举原理

我们知道Zookeeper集群中也有选举机制，是通过Paxos算法，通过不同节点向其他节点发送信息来投票选举出leader，但是Kafka的leader的选举就没有这么复杂了。 
Kafka的Leader选举是通过在zookeeper上创建/controller临时节点来实现leader选举，并在该节点中写入当前broker的信息 
{“version”:1,”brokerid”:1,”timestamp”:”1512018424988”} 
利用Zookeeper的强一致性特性，一个节点只能被一个客户端创建成功，创建成功的broker即为leader，即先到先得原则，leader也就是集群中的controller，负责集群中所有大小事务。 

当leader和zookeeper失去连接时，临时节点会删除，而其他broker会监听该节点的变化，当节点删除时，其他broker会收到事件通知，重新发起leader选举。

#### 如何确保消息不丢失

设置acks=all：

写完后再回传一个ack信息；

~~

关闭 RabbitMQ 的自动提交offset：

等处理完了再手动提交；

# RabbitMq

#### 交换机（Exchange）

direct(默认)，fanout, topic, 和headers

路由健完全匹配，不处理路由健，路由健模糊匹配，基于消息的header数据匹配；

#### RabbitMq集群：

 RabbitMQ 集群分为两种 普通集群 和 镜像集群。

普通集群：A收到消息 B消费，则要从A上去拿；

镜像集群：主从架构；

RabbitMQ 的集群节点包括内存节点、磁盘节点。顾名思义内存节点就是将所有数据放在内存，磁盘节点将数据放在磁盘。

#### rabbitmq 每个节点是其他节点的完整拷贝吗？为什么？

普通集群则不是；镜像集群是；

#### 如何确保消息不丢失

![rabbitmq-message-lose-solution](https://github.com/doocs/advanced-java/raw/master/images/rabbitmq-message-lose-solution.png)

#### 保证幂等性（不被重复消费）

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 直接用把offset写入redis中，看是否消费过； 

#### 如何保证消息的顺序性？

写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。

#### 和RabbitMq对比：

高吞吐量；

时效性不如rabbitmq；

可用性架构不同：rabbitmq是基于主从架构，kafka基于分布式架构；

kafka 唯一的一点劣势是有可能消息重复消费；

# 海量数据处理

### 1.海量日志数据，提取出某日访问百度次数最多的那个IP

1. 分而治之/hash映射：针对数据太大，内存受限，只能是把大文件化成(取模映射)小文件；按照IP地址的Hash(IP)%1000值，把海量IP日志分别存储到1000个小文件中。这样，每个小文件最多包含4MB个IP地址
2. hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。

### 2.寻找热门查询，300万个查询字符串中统计最热门的10个查询

一个字符串1~255字节；300 * 10^4 * 255/ 2^30=0.75G直接放在内存中hashmap处理；后进行堆排序；

### 3.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

分为1000个小文件，每个都用hashmap统计频率，取出最高的100个，然后堆排序；

### 4.海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。

顺序读取10个文件，hashmap每台机器取前100，然后堆排序；

### 5.有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。

顺序读取10个文件，按照hash%10写入到另外10个文件，hashmap统计，快速排序

### 6.给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

a，b文件分别遍历按hash分给100个小文件，hashset对比是否有相同的url；

### 7.在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。

采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存

### 8.给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。















