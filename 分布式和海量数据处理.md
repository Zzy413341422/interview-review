# 分布式和和消息队列和海量数据处理

# RabbitMq

![](C:/Users/admin/Desktop/interview-review/md/90.png)
Exchange：消息交换机,它指定消息按什么规则,路由到哪个队列。 
Queue:消息的载体,每个消息都会被投到一个或多个队列。 
Binding:绑定，它的作用就是把exchange和queue按照路由规则绑定起来. 
Routing Key:路由关键字,exchange根据这个关键字进行消息投递。 

#### 为什么使用消息队列

异步、解耦、削峰

#### 交换机（Exchange）

direct(默认)，fanout, topic, 和headers

路由健完全匹配，不处理路由健，路由健模糊匹配，基于消息的header数据匹配；

三种模式 direct fanout topic，在 direct 模式下明显消息发布的性能比其他模式强很多

#### 消息队列的缺点

系统可用性降低：镜像集群

系统复杂度提高：需要考虑消息重复消费、消息丢失、甚至消息顺序性

数据一致性问题

#### RabbitMq集群：

 RabbitMQ 集群分为两种 普通集群 和 镜像集群。

普通集群：A收到消息 B消费，则要从A上去拿；

镜像集群：主从架构；

#### CMQ集群

CMQ中，主要是实现主从集群。Raft 协议强依赖 Leader 节点的可用性来确保集群数据的一致性。数据的流向只能从Leader 节点向 Follower 节点转移。

#### rabbitmq 每个节点是其他节点的完整拷贝吗？为什么？

普通集群则不是；镜像集群是；

#### 如何确保消息不丢失

![img](C:/Users/admin/Desktop/interview-review/md/54.png)

#### 保证幂等性（不被重复消费）

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 直接用把offset写入redis中，看是否消费过； 

#### 如何保证消息的顺序性？

如果存在多个消费者，那么就让每个消费者对应一个queue，然后把要发送 的数据全都放到一个queue，这样就能保证所有的数据只到达一个消费者从而保证每个数据到达数据库都是顺序的。

#### 如何防止处理过程中异常而导致的消息丢失？

消费者拉取消息时会指定当前消息的隐藏时间，在隐藏时 间内消费者需要显式的对消息进行确认删除（ACK），消息才会真真的被删除。如果超过隐藏时间没有主动删除， 此条消息将重新对外可见，可以继续消费。三种模式 direct fanout topic，在 direct 模式下明显消息发布的性能比其他模式强很多

#### 百万数据积压

1、先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉。

2、新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量。

3、然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue

4、接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据

5、这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据

6、等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息

# Kafka

#### Kafka简介

![](C:/Users/admin/Desktop/interview-review/md/99.png)

一、Kafka是一个分布式消息队列系统，以集群方式存在。

优点：高吞吐率（每秒百万级）；基于多分区多副本实现高容错，并发能力强；易扩展（增加broker）等。

#### Kafka原理

Kafka依赖zk，以集群方式工作，每台机器称为一个brocker，并分别指定brockerId，kafka中同一类型数据以topic形式存在，可对topic进行分区，及指定副本数，例如可将 topic1 分区为3个partition:p0,p1,p2 ;对于每个partition，send(topic, key, data); key&size选择partition；

1个partition只能被处于同一消费组者组的一个consumer消费，同组的consumer则起到均衡效果

#### 推还是拉？

推的话消费速度更快，但容易推的消息速度过大，造成comsumer崩溃。

#### Kafka集群Leader选举原理

Kafka的Leader选举是通过在zookeeper上创建/controller临时节点来实现leader选举，并在该节点中写入当前broker的信息 ：{“version”:1,”brokerid”:1,”timestamp”:”1512018424988”} 
利用Zookeeper的强一致性特性，一个节点只能被一个客户端创建成功，创建成功的broker即为leader，即先到先得原则，leader也就是集群中的controller，负责集群中所有大小事务。

#### kafka主备策略

Kafka通过replica.lag.max.messages和replica.lag.time.max.ms两个参数来度量Follower同步的情况。其中replica.lag.max.messages用于配置Follower可落后的最大消息数量，replica.lag.time.max.ms用于配置Follower和Leader通信的最大时延。超过后会被踢出ISR，直到追上后加回到ISR列表

#### offset保存策略

使用zookeeper来维护offset，kafka 0.9 以前的版本是将offset 存储在zookeeper上的。kafka 0.9 以后，offset的使用了内部的roker来管理。

#### Rebalance分配策略（partion分配给consumer的过程）

1、轮询的方式分配所有的分区

2、按分区数/消费者数来分配。

3、Sticky：首先将分区根据所分配的consumer数量从低到高进行排序，后将最前的分区分给订阅了的且分配了最少分区的消费者。

#### 如何保证消息的顺序性？

生产者生产消息只发到一个partation ，单partation有序，多partation无序，Kafka 只会保证在 Partition 内消息是有序的，而不管全局的情况。

#### 交付语义保证

Kafka可以提供的消息交付语义保证有多种：

At most once——消息可能会丢失但绝不重传。（异步传输）
At least once——消息可以重传但绝不丢失。（同步传输）
Exactly once——这正是人们想要的, 每一条消息只被传递一次。（producer 给每条被发送的消息分配了一个序列号来避免产生重复的消息）

#### 下次面试官问我 kafka 为什么快，我就这么说

1、partition 并行处理，消费者组模式
2、顺序写磁盘，顺序写磁盘的性能是随机写入的性能的6000倍的提升，媲美内存随机访问的性能，磁盘不再是瓶颈点。
3、Broker 收到数据后，写磁盘时只是将数据写入 Page Cache，Kafka的读写操作基本上是基于内存的，并不保证数据一定完全写入磁盘。
4、采用了零拷贝技术，
	Producer 生产的数据持久化到 broker，采用 mmap 文件映射，实现顺序的快速写入内存，由操作系统异步写入磁盘。并使用sendfile发送到socket缓存区和网卡缓冲区。
	一个 topic 被多消费者消费，从磁盘中读取数据，数据在使用时只会被复制到 pagecache 中一次，节省了每次拷贝到用户空间内存中，再从用户空间进行读取的消耗。

#### 和RabbitMq对比：

在架构模型方面：AMQP：Exchange,Binding,queue、、一般的MQ结构，broker

吞吐量：kafka使用page cache+ mmap，rabbitmq使用内存，rabbitmq通过后台线程刷盘，kafka通过系统内核来进行刷盘。

# 注册中心

一致性与可用性：

剔除坏节点：

服务地址变更监听和自动推送。

需要建立大量的长连接。

### 假死脑裂是什么？如何解决？

master节点宕机，选举了新的节点，宕机节点恢复，存在2个master节点。

如何避免：在follower切换的时候不在检查到老的master出现问题后马上切换，而是在休眠一段足够的时间，确保老的master已经获知变更并且做了相关的shutdown清理工作了然后再注册成为master

## RPC

网络：IO多路复用，假如是Java通信，可用Netty实现。

反射：服务名+方法名+参数通过Java反射找到对应的方法并执行。

序列化：gson、fastjson等等。

协议：HTTP、DUBBO、PROTOBUF、HESSION等tcp之上的协议都可以用来做通信协议。

负载均衡：随机、轮询、按权重轮询等等。

缓存服务地址：

容错熔断：

grpc是把数据依照pb协议编码然后利用http2协议进行传输的一套技术方案的统称

## hystrix与sentinel的区别

|              | Sentinel                       | Hystrix               |
| ------------ | ------------------------------ | --------------------- |
| 隔离策略     | 信号量隔离（并发线程数限流）   | 线程池隔离/信号量隔离 |
| 熔断降级策略 | 基于响应时间、异常比率、异常数 | 基于异常比率          |
| 实时统计实现 | 滑动窗口                       | 滑动窗口              |

Sentinel  信号量隔离（并发线程数限流）

Hystrix    线程池隔离/信号量隔离

## Nginx

![img](C:/Users/admin/Desktop/interview-review/md/60.png)

- **多进程：**一个 Master 进程、多个 Worker 进程。
- **Master 进程：**管理 Worker 进程。对外接口：接收外部的操作（信号）；对内转发：根据外部的操作的不同，通过信号管理 Worker；**监控：**监控 Worker 进程的运行状态，Worker 进程异常终止后，自动重启 Worker 进程。
- **Worker 进程：**所有 Worker 进程都是平等的。实际处理：网络请求，由 Worker 进程处理。Worker 进程数量**：**在 nginx.conf 中配置，一般设置为核心数，充分利用 CPU 资源，同时，避免进程数量过多，避免进程竞争 CPU 资源，增加上下文切换的损耗。

### nginx的处理流程

Nginx的IO通常使用epoll，epoll函数使用了I/O复用模型。与I/O阻塞模型比较，I/O复用模型的优势在于可以同时等待多个（而不只是一个）套接字描述符就绪。Nginx的epoll工作流程如下：

1.首先，master 进程接受到信号（如nginx -s reload）后启动，读取配置文件，建好需要listen的socket后，然后再fork出多个woker进程，这样每个work进程都可以去accept这个socket

2.当一个client连接到来时，所有accept的work进程都会受到通知，但只有一个进程可以accept成功，其它的则会accept失败，Nginx提供了一把共享锁accept_mutex来保证同一时刻只有一个work进程在accept连接，从而解决惊群问题

3.当一个worker进程accept这个连接后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完成的请求就结束了

## Maven

主要区别如下:
package命令完成了项目编译、单元测试、打包功能，但没有把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库

install命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库，但没有布署到远程maven私服仓库

deploy命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库.

# 分布式

## 为什么要使用分布式

1.提高可用性；

2.易扩展功能；

3.易于开发，每个服务都可独立部署，一个开发人员只须负责一个服务开发；

4.便于提升性能；

## CAP理论

一致性：保持各节点数据一致；

可用性：就是无论什么情况，都是可以访问的，不会宕机。

分区容忍性：简单可以理解为，多副本的分布存储。

## BASE理论

BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性，但应用可以采用适合的方式达到最终一致性。

基本可用：基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。

软状态：软状态是指允许系统存在中间状态

最终一致性：最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

#### 强一致性：在任意时刻，所有节点中的数据是一样的。

#### 弱一致性：就是不保证在任意时刻任意节点上的同一份数据都是相同的

数据库主库和从库不一致，常见有这么几种优化方案：
（1）业务可以接受，系统不优化
（2）强制读主，高可用主库，用缓存提高读性能

## 分布式一致性协议

### 2pc

![](md\50.png)

缺点：1）协调者单点。

2）数据不一致：二阶段执行成功失败没有返回。

3）阻塞资源：二阶段参与者收不到commit

### 3pc

![](md\123.png)

改善：减少阻塞资源，第三阶段参与者会默认执行commit，释放资源。

缺点：单点仍存在

### paxos

选举：![](md\124.png)（保证数据尽可能最新且连同原则）

当 Leader 收到写操作时，先本地生成事务为事务生成 zxid，然后发给所有 Follower 节点。
当 Follower 收到事务时，先把提议事务的日志写到本地磁盘，成功后返回给 Leader。
Leader 收到过半反馈后对事务提交。再通知所有的 Follower 提交事务， Follower 收到后也提交事务，提交后就可以对客户端进行分发了。

### raft

选举：*Raft*采用心跳机制来触发Leader*选举*。Follower递增自己的任期并设置为Candidate角色。投票给自己并且并发的给所有节点发送投票请求。同一个任期内获得大多数选票，成为Leader（一个节点在一个任期内只能给一个Candidate投票，任期相同则选票先到先得）。（最快恢复且连同原则）

每次数据同步操作同时也是一个心跳，会更新 Follower 的 election timeout。另外只有当多数节点返回同步成功之后，Leader 才会给客户端返回操作成功。

#### Zab

选举：

每个Follower节点都向所有其它节点广播Vote投票请求，即请求自己成为Leader；
如果Follower接受到的Vote请求中的ZXID比自身的大，则投票同意，并更新自身的Vote，否则拒绝；
每个Follower都维护着一个投票记录表，当某个Follower节点收到过半的选票时，结束投票并把该Follower选为Leader。（保证数据尽可能最新且连同原则）

数据同步同paxos。

## 中间件一致性策略

mysql同步机制

1.对于异步复制，主库将事务Binlog事件写入到Binlog文件中，此时主库只会通知一下Dump线程发送这些新的Binlog，然后主库就会继续处理提交操作，而此时不会保证这些Binlog传到任何一个从库节点上。
2.对于全同步复制，当主库提交事务之后，所有的从库节点必须收到，APPLY并且提交这些事务，然后主库线程才能继续做后续操作。这里面有一个很明显的缺点就是，主库完成一个事务的时间被拉长，性能降低。
3.对于半同步复制，是介于全同步复制和异步复制之间的一种，主库只需要等待至少一个从库节点（可配置）收到并且Flush Binlog到Relay Log文件即可，主库不需要等待所有从库给主库反馈。同时，这里只是一个收到的反馈，而不是已经完全执行并且提交的反馈，这样就节省了很多时间。

Zookeeper的zab策略脱胎于Paxos算法，默认情况下，zk中写数据时，要有一半以上的从节点写入成功，才算是写入成功。
redis因为是要提升性能，所以直接采用的异步复制，当在Master上写入数据后直接返回，然后把数据快照广播给Slave，让所有的Slaves去执行操作
Rabbitmq同步策略：

all 所有的节点都将被同步

exactly 指定个数的节点被同步

nodes 指定的名称的节点被同步

kafka使用的是ISR，只有 High Water Mark 以上的消息才支持 Consumer 读取。

- `acks=0` 如果设置为0，则 producer 不会等待服务器的反馈。该消息会被立刻添加到 socket buffer 中并认为已经发送完成。在这种情况下，服务器是否收到请求是没法保证的，并且参数`retries`也不会生效（因为客户端无法获得失败信息）。每个记录返回的 offset 总是被设置为-1。
- `acks=1` 如果设置为1，leader节点会将记录写入本地日志，并且在所有 follower 节点反馈之前就先确认成功。在这种情况下，如果 leader 节点在接收记录之后，并且在 follower 节点复制数据完成之前产生错误，则这条记录会丢失。
- `acks=all` 如果设置为all，这就意味着 leader 节点会等待所有同步中的副本确认之后再确认这条记录是否发送完成。只要至少有一个同步副本存在，记录就不会丢失。这种方式是对请求传递的最有效保证。acks=-1与acks=all是等效的。

更少的可容错副本数量。raft当有1/3个节点数目不可用时，服务不可用，kafka中维护的ISR（in-sync-replica），即便副本均不可用，只剩下Leader，也可以对外提供服务

es有三种一致性参数：
1.one：要求我们这个写操作，只要有一个primary shard是active活跃可用的，就可以执行
2.all：要求我们这个写操作，必须所有的primary shard和replica shard都是活跃的，才可以执行这个写操作
3.quorum：默认的值，要求所有的shard中，必须是大部分的shard都是活跃的，可用的，才可以执行这个写操作

## 分布式锁

#### redis分布式锁：

RedLock算法：

1. 获取当前时间戳，单位是毫秒；
2. 尝试在**大多数节点**上建立一个锁，比如 5 个节点就要求是 3 个节点 `n / 2 + 1`；
3. 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了；
4. 要是锁建立失败了，那么就依次之前建立过的锁删除；
5. 只要别人建立了一把分布式锁，你就得**不断轮询去尝试获取锁**。

缺陷：强依赖于时间，ABCDE，ABC中的B先过期了，BDE就可以获得锁，存在两个锁。

redis普通锁：![](C:/Users/admin/Desktop/interview-review/md/84.png)

缺陷：master宕机前写入myLock，salve还没有同步到这把锁，就被切换成了master，另一线程也写入了myLoCK。

#### zk 分布式锁：

#### redis 分布式锁和 zk 分布式锁的对比：

redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。并且redis锁实现了高级锁特性
zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。

## 分布式事务

#### 具体场景

本地事务和发消息如何保证强一致性

![](C:/Users/admin/Desktop/interview-review/md/49.png)

### TCC 方案

TCC 的全称是：`Try`、`Confirm`、`Cancel`。

- Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行**锁定或者预留**。
- Confirm 阶段：这个阶段说的是在各个服务中**执行实际的操作**。
- Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要**进行补偿**，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）

# 海量数据处理

### 1.海量日志数据，提取出某日访问百度次数最多的那个IP

1. 分而治之/hash映射：针对数据太大，内存受限，只能是把大文件化成(取模映射)小文件；按照IP地址的Hash(IP)%1000值，把海量IP日志分别存储到1000个小文件中。这样，每个小文件最多包含4MB个IP地址
2. hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。

### 2.寻找热门查询，300万个查询字符串中统计最热门的10个查询

一个字符串1~255字节；300 * 10^4 * 255/ 2^30=0.75G直接放在内存中hashmap处理；后进行堆排序；

### 3.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

分为1000个小文件，每个都用hashmap统计频率，取出最高的100个，然后堆排序；

### 4.海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。

顺序读取10个文件，hashmap每台机器取前10，然后堆排序；

### 5.有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。

顺序读取10个文件，按照hash%10写入到另外10个文件，hashmap统计，快速排序

### 6.给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

a，b文件分别遍历按hash分给100个小文件，hashset对比是否有相同的url；

### 7.在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。

采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存

### 8.给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

### MapReduce

![](md\97.png)

### 布隆过滤器

![img](md\61.png)



那么我们可以说 “baidu” 存在了么？答案是不可以，只能是 “baidu” 这个值可能存在。











